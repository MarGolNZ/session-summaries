{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8AZm1jRlY_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Understanding derivatives\n",
        "# https://colab.research.google.com/\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocnzInIgmnhN",
        "colab_type": "code",
        "outputId": "4d420760-6bda-4908-f9aa-dcd497862512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'diabetes.csv.gz', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cwrS0XTmovr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kASE4cmV0Ljm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSJx3ANH0VvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzWjQIS30Zv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('diabetes.csv.gz', compression='gzip', header=None, dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f_7k71Y0jKf",
        "colab_type": "code",
        "outputId": "0f0330f5-af98-4103-9c1e-d347139e23b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.294118</td>\n",
              "      <td>0.487437</td>\n",
              "      <td>0.180328</td>\n",
              "      <td>-0.292929</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001490</td>\n",
              "      <td>-0.531170</td>\n",
              "      <td>-0.033333</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.882353</td>\n",
              "      <td>-0.145729</td>\n",
              "      <td>0.081967</td>\n",
              "      <td>-0.414141</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.207153</td>\n",
              "      <td>-0.766866</td>\n",
              "      <td>-0.666667</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.058823</td>\n",
              "      <td>0.839196</td>\n",
              "      <td>0.049180</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.305514</td>\n",
              "      <td>-0.492741</td>\n",
              "      <td>-0.633333</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.882353</td>\n",
              "      <td>-0.105528</td>\n",
              "      <td>0.081967</td>\n",
              "      <td>-0.535354</td>\n",
              "      <td>-0.777778</td>\n",
              "      <td>-0.162444</td>\n",
              "      <td>-0.923997</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.376884</td>\n",
              "      <td>-0.344262</td>\n",
              "      <td>-0.292929</td>\n",
              "      <td>-0.602837</td>\n",
              "      <td>0.284650</td>\n",
              "      <td>0.887276</td>\n",
              "      <td>-0.600000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>754</th>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.015075</td>\n",
              "      <td>0.245902</td>\n",
              "      <td>-0.030303</td>\n",
              "      <td>-0.574468</td>\n",
              "      <td>-0.019374</td>\n",
              "      <td>-0.920581</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>755</th>\n",
              "      <td>-0.764706</td>\n",
              "      <td>0.226131</td>\n",
              "      <td>0.147541</td>\n",
              "      <td>-0.454545</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.096870</td>\n",
              "      <td>-0.776260</td>\n",
              "      <td>-0.800000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>756</th>\n",
              "      <td>-0.411765</td>\n",
              "      <td>0.216080</td>\n",
              "      <td>0.180328</td>\n",
              "      <td>-0.535354</td>\n",
              "      <td>-0.735225</td>\n",
              "      <td>-0.219076</td>\n",
              "      <td>-0.857387</td>\n",
              "      <td>-0.700000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>757</th>\n",
              "      <td>-0.882353</td>\n",
              "      <td>0.266332</td>\n",
              "      <td>-0.016393</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.102832</td>\n",
              "      <td>-0.768574</td>\n",
              "      <td>-0.133333</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758</th>\n",
              "      <td>-0.882353</td>\n",
              "      <td>-0.065327</td>\n",
              "      <td>0.147541</td>\n",
              "      <td>-0.373737</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.093890</td>\n",
              "      <td>-0.797609</td>\n",
              "      <td>-0.933333</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>759 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2         3  ...         5         6         7    8\n",
              "0   -0.294118  0.487437  0.180328 -0.292929  ...  0.001490 -0.531170 -0.033333  0.0\n",
              "1   -0.882353 -0.145729  0.081967 -0.414141  ... -0.207153 -0.766866 -0.666667  1.0\n",
              "2   -0.058823  0.839196  0.049180  0.000000  ... -0.305514 -0.492741 -0.633333  0.0\n",
              "3   -0.882353 -0.105528  0.081967 -0.535354  ... -0.162444 -0.923997  0.000000  1.0\n",
              "4    0.000000  0.376884 -0.344262 -0.292929  ...  0.284650  0.887276 -0.600000  0.0\n",
              "..        ...       ...       ...       ...  ...       ...       ...       ...  ...\n",
              "754  0.176471  0.015075  0.245902 -0.030303  ... -0.019374 -0.920581  0.400000  1.0\n",
              "755 -0.764706  0.226131  0.147541 -0.454545  ...  0.096870 -0.776260 -0.800000  1.0\n",
              "756 -0.411765  0.216080  0.180328 -0.535354  ... -0.219076 -0.857387 -0.700000  1.0\n",
              "757 -0.882353  0.266332 -0.016393  0.000000  ... -0.102832 -0.768574 -0.133333  0.0\n",
              "758 -0.882353 -0.065327  0.147541 -0.373737  ... -0.093890 -0.797609 -0.933333  1.0\n",
              "\n",
              "[759 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yBn4FMD0nB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLi3WrHr03YN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,0:8],\n",
        "                                                    df.iloc[:,8], random_state=8 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSdxEzNS1Ipj",
        "colab_type": "code",
        "outputId": "70c3fa3d-08da-4e70-9273-e9d21847883a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((569, 8), (190, 8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laEDRwB46S0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = torch.from_numpy(X_train.values)\n",
        "X_test = torch.from_numpy(X_test.values)\n",
        "y_train = torch.from_numpy(y_train.values)\n",
        "y_test = torch.from_numpy(y_test.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHiak8ud6q9r",
        "colab_type": "code",
        "outputId": "9753ab41-fdbf-4405-9b3f-792ba2fd0d62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.dtype"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRQG1MtJ1OCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(8,16)\n",
        "        self.l2 = nn.Linear(16,22)\n",
        "        self.l3 = nn.Linear(22,1)\n",
        "    def forward(self,x):\n",
        "        x1 = torch.relu(self.l1(x))\n",
        "        x2 = torch.relu(self.l2(x1))\n",
        "        x3 = torch.sigmoid(self.l3(x2))\n",
        "        return x3\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ez75OoT1KkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(3,10)\n",
        "        self.l2 = nn.Linear(10,1)\n",
        "    def forward(self,x):\n",
        "        return self.l1(x)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-1NlWQv4Db2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(truth, preds):\n",
        "    return (truth == preds).float().mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLTdVG-v61xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3NR_CE47KeL",
        "colab_type": "code",
        "outputId": "a67430d5-def3-444d-ef7d-9cc0e3bdbd09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "list(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.2705,  0.0564,  0.2240, -0.1739, -0.2088,  0.1190, -0.3208, -0.0908],\n",
              "         [ 0.1587,  0.1620,  0.1861, -0.0203,  0.0742,  0.3273, -0.3309, -0.1204],\n",
              "         [-0.0736, -0.2303, -0.2425,  0.2312, -0.2762,  0.1173, -0.1089, -0.1654],\n",
              "         [-0.2091,  0.3450, -0.1660, -0.1198, -0.3388, -0.0917,  0.3044,  0.0532],\n",
              "         [ 0.2128,  0.3139, -0.0062,  0.0932,  0.2798,  0.1558, -0.2298, -0.2112],\n",
              "         [ 0.2966,  0.1710,  0.1405,  0.1134, -0.1843, -0.1430, -0.3352, -0.2998],\n",
              "         [-0.0251,  0.3137, -0.3382, -0.3471,  0.0951, -0.3513, -0.1995,  0.3220],\n",
              "         [-0.0723,  0.1397, -0.0922, -0.0748,  0.1307,  0.1339, -0.0739,  0.3411],\n",
              "         [-0.2699,  0.0661,  0.2806, -0.3526, -0.1465, -0.0147, -0.0545, -0.1002],\n",
              "         [ 0.0404,  0.2012,  0.0016, -0.2944,  0.2509,  0.2755, -0.2330, -0.0783],\n",
              "         [ 0.2613, -0.0157, -0.0548,  0.0924, -0.2571,  0.2294,  0.3347, -0.0603],\n",
              "         [ 0.2093,  0.0045,  0.1050,  0.3089,  0.0981, -0.1156,  0.2753,  0.2899],\n",
              "         [-0.2485,  0.2923, -0.0660, -0.0610, -0.0712,  0.0489,  0.3251, -0.1341],\n",
              "         [ 0.0791,  0.2879, -0.3012, -0.0004,  0.2122, -0.1874,  0.2344,  0.3200],\n",
              "         [ 0.1942, -0.2325, -0.2206,  0.1890, -0.2890, -0.0245,  0.2645, -0.1244],\n",
              "         [ 0.0834, -0.2764, -0.2461,  0.2332, -0.2172, -0.3133, -0.2096,  0.3189]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 0.1965, -0.2690,  0.0381,  0.0838, -0.3024, -0.1743,  0.0320, -0.2399,\n",
              "          0.1084, -0.2706,  0.2603,  0.0340, -0.1972,  0.0203, -0.0342, -0.2662],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[ 1.8008e-01,  2.4298e-01,  2.2693e-01,  1.5913e-01, -1.8243e-01,\n",
              "          -7.4733e-02, -5.1469e-02, -1.4327e-02, -1.4035e-01, -1.5988e-02,\n",
              "          -1.7087e-01,  4.0147e-02,  5.8856e-02,  1.3095e-04,  2.1850e-01,\n",
              "           7.3278e-02],\n",
              "         [-1.0596e-01,  1.6656e-01, -1.4185e-01, -1.9512e-01,  1.0171e-01,\n",
              "          -1.0597e-01, -1.1287e-01, -4.7116e-02, -1.1738e-01,  2.3227e-01,\n",
              "          -1.4996e-01,  2.1828e-01,  2.7432e-02, -9.6258e-02, -1.9802e-01,\n",
              "           9.3111e-02],\n",
              "         [ 8.1649e-02, -2.3909e-01, -4.5724e-02, -1.5036e-01,  3.2216e-02,\n",
              "          -1.2964e-01, -8.4211e-03,  1.6800e-01,  6.3472e-02,  6.4470e-02,\n",
              "          -1.0019e-01,  3.0464e-02,  2.2836e-02, -2.0943e-01, -2.9406e-02,\n",
              "          -5.5332e-02],\n",
              "         [-2.7244e-02,  6.9594e-02,  8.6208e-02,  1.5385e-01, -6.6700e-02,\n",
              "           2.0210e-01,  5.5232e-02, -1.3730e-01,  8.3553e-02, -2.2583e-01,\n",
              "           2.1296e-01, -1.1084e-01, -5.3199e-02,  1.5937e-01, -7.1574e-02,\n",
              "          -1.8110e-01],\n",
              "         [-2.9139e-02, -5.9299e-03,  1.1654e-01,  1.2170e-01,  2.3059e-01,\n",
              "           1.9864e-01,  1.6573e-01, -1.4858e-01,  2.8244e-02, -2.3717e-01,\n",
              "           1.3292e-01,  2.0421e-01,  7.6759e-02, -1.5765e-03, -8.4637e-02,\n",
              "          -2.1760e-01],\n",
              "         [ 1.8425e-01, -1.1329e-01,  9.5677e-03,  2.5332e-02,  1.6676e-01,\n",
              "          -6.5908e-02, -1.0074e-01,  1.3194e-01, -1.7234e-01, -2.6140e-02,\n",
              "          -2.2131e-01, -2.1741e-01,  1.3729e-01,  2.3286e-01, -1.3925e-01,\n",
              "          -2.1422e-01],\n",
              "         [-2.0315e-01, -1.7152e-01, -1.3656e-01, -1.3287e-02,  1.9287e-01,\n",
              "           6.9166e-02,  1.4082e-01,  2.1559e-01, -1.6601e-01,  1.9661e-01,\n",
              "          -7.7198e-02, -2.4459e-01, -1.2844e-02, -2.5079e-02,  4.5020e-02,\n",
              "           4.5645e-02],\n",
              "         [-2.1518e-01,  1.6257e-01,  1.7778e-01, -2.0626e-01, -1.8566e-02,\n",
              "           2.4643e-01,  1.6727e-01, -1.9594e-01,  9.2746e-02,  7.5401e-02,\n",
              "           1.7188e-01, -3.2680e-02,  9.1642e-02,  2.4646e-01, -2.3273e-01,\n",
              "          -1.7664e-01],\n",
              "         [-1.1271e-01,  2.0482e-01,  2.1069e-01, -1.4933e-01, -1.9275e-01,\n",
              "          -6.2717e-02,  1.8099e-01,  8.1094e-02,  2.1166e-01, -9.8687e-02,\n",
              "           1.8724e-01, -2.1061e-01,  2.2542e-01,  2.7767e-02,  8.5141e-02,\n",
              "           1.2511e-01],\n",
              "         [-8.0701e-02, -2.4445e-01,  8.4700e-02, -2.1483e-01, -3.7042e-02,\n",
              "           2.2793e-01, -2.0786e-02, -7.3396e-02, -2.4585e-01,  2.3063e-02,\n",
              "           5.6056e-02,  1.9960e-01,  9.8635e-02, -1.2773e-01, -3.6943e-02,\n",
              "           5.7729e-03],\n",
              "         [ 2.1056e-01,  9.0857e-02,  9.7538e-02,  1.8161e-01,  2.1623e-01,\n",
              "           5.9505e-02, -1.0489e-01, -5.4224e-02,  3.3001e-02,  2.1608e-02,\n",
              "          -9.0722e-02,  1.8223e-01,  2.2431e-01, -1.1946e-01, -2.0643e-01,\n",
              "          -1.8692e-02],\n",
              "         [-4.8277e-02, -1.9046e-01, -5.8388e-02, -1.0428e-01, -5.5869e-02,\n",
              "           4.3992e-02, -1.1094e-01, -1.3568e-01, -2.2026e-01,  1.7608e-01,\n",
              "           9.6340e-02, -1.8167e-02, -3.2413e-02, -1.4140e-01,  1.7742e-01,\n",
              "          -2.3773e-01],\n",
              "         [ 5.0319e-02, -1.2603e-01, -8.3199e-02, -2.3969e-01, -1.6480e-01,\n",
              "          -1.9411e-01,  1.6006e-01,  2.1880e-01, -3.4725e-02, -2.4083e-01,\n",
              "          -1.0348e-01,  1.3779e-01,  1.0038e-01, -1.5054e-01, -1.3881e-01,\n",
              "          -5.2996e-02],\n",
              "         [-6.8548e-02,  1.9484e-01,  1.6076e-01, -1.3978e-01,  1.3496e-01,\n",
              "           8.8704e-02,  3.5445e-02, -1.3764e-01, -1.0712e-01,  1.4183e-01,\n",
              "           3.5857e-02,  1.2030e-01,  1.3409e-01, -1.8037e-01, -1.7491e-01,\n",
              "           1.8956e-01],\n",
              "         [ 1.7477e-01,  4.0153e-02, -1.7285e-01, -5.6249e-02,  4.2301e-02,\n",
              "           2.3505e-01, -2.2621e-01, -1.3366e-01, -2.0711e-01, -8.7031e-02,\n",
              "          -9.4882e-02,  2.3786e-01,  1.7851e-01,  1.6651e-01,  1.5143e-01,\n",
              "           2.2318e-01],\n",
              "         [ 2.4566e-01, -2.1323e-01,  1.7459e-01,  2.0486e-01, -1.3830e-01,\n",
              "          -2.1039e-01, -2.0851e-01,  1.8799e-01,  1.7511e-01, -1.8694e-01,\n",
              "           2.1154e-01, -6.3850e-02, -2.3606e-02, -9.4175e-02, -2.0287e-01,\n",
              "           1.6261e-01],\n",
              "         [-1.1062e-01,  1.2853e-01, -2.4622e-01, -2.0290e-01, -1.4081e-01,\n",
              "           1.6717e-01, -1.9403e-02, -1.9010e-01, -6.4125e-02,  9.2728e-02,\n",
              "           1.1176e-01,  2.1292e-01, -2.2739e-01,  1.6892e-01, -1.2879e-01,\n",
              "          -8.3956e-02],\n",
              "         [-2.2901e-01, -1.6224e-01, -1.2046e-02, -1.4316e-01, -1.3481e-01,\n",
              "          -8.0583e-02,  2.4580e-01, -2.2345e-01, -2.1542e-01,  1.6274e-01,\n",
              "           7.0123e-02,  8.0200e-02, -2.2510e-01,  1.2108e-01, -4.6723e-02,\n",
              "          -2.1188e-01],\n",
              "         [-1.6501e-01, -9.9399e-02,  1.1419e-01, -2.3121e-02, -1.2781e-01,\n",
              "          -2.1447e-01,  1.0142e-01,  1.0748e-01,  2.0848e-02, -2.2465e-01,\n",
              "           8.5422e-02,  1.3236e-01, -5.2217e-02,  2.8583e-03, -2.2193e-01,\n",
              "          -1.9921e-01],\n",
              "         [-1.9595e-01, -5.5833e-02,  1.0380e-02,  2.0465e-01,  1.7669e-01,\n",
              "          -9.9990e-02,  2.0599e-01, -1.4207e-01, -2.3933e-01,  3.4566e-02,\n",
              "          -7.1965e-02, -2.4903e-01, -3.1542e-02, -7.5100e-02, -1.1818e-01,\n",
              "          -5.3401e-02],\n",
              "         [-5.8984e-02, -2.4594e-01, -1.3240e-01,  1.8214e-01, -1.0085e-01,\n",
              "          -2.3210e-01,  1.9977e-01,  3.9125e-02, -1.4264e-01,  3.7779e-02,\n",
              "          -2.0114e-01, -2.2883e-01, -1.0594e-01,  6.8152e-02, -1.4403e-01,\n",
              "           1.2556e-01],\n",
              "         [ 3.6650e-02,  2.0269e-01, -1.9020e-02,  5.9013e-02,  1.7174e-01,\n",
              "          -1.2646e-01,  5.3395e-02,  2.2233e-01,  5.5947e-02, -2.7661e-02,\n",
              "           4.2542e-02,  2.2774e-01, -5.5455e-02, -1.9813e-02,  1.4401e-01,\n",
              "          -1.8603e-01]], requires_grad=True), Parameter containing:\n",
              " tensor([-0.2309,  0.0228, -0.1322, -0.1717,  0.1996,  0.2454,  0.1082, -0.0941,\n",
              "         -0.2183, -0.2480, -0.1841, -0.1987, -0.0086, -0.0829, -0.0256, -0.2406,\n",
              "         -0.1964,  0.0046, -0.0336, -0.1419, -0.2485, -0.0073],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[-0.0955,  0.1784,  0.0824, -0.0832,  0.2028,  0.1227, -0.1803,  0.0631,\n",
              "           0.1902,  0.0892,  0.0028, -0.1106,  0.0399, -0.2075, -0.1703, -0.1191,\n",
              "          -0.0736,  0.2124,  0.0185,  0.1147,  0.0242,  0.1010]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([-0.1058], requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d8eNzrp7X6i",
        "colab_type": "code",
        "outputId": "39d6b370-bf9a-452c-93f6-74aad045a6c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "for epoch in range(501):\n",
        "    y_pred = model.forward(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_test = model.forward(X_test)\n",
        "        pred_test = pred_test >0.5\n",
        "        acc = accuracy(y_test, pred_test.view(y_test.shape))\n",
        "        if epoch%50 ==0:\n",
        "            print(epoch, acc)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([569])) that is different to the input size (torch.Size([569, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.3053)\n",
            "50 tensor(0.6842)\n",
            "100 tensor(0.7105)\n",
            "150 tensor(0.7842)\n",
            "200 tensor(0.7947)\n",
            "250 tensor(0.8053)\n",
            "300 tensor(0.8053)\n",
            "350 tensor(0.8158)\n",
            "400 tensor(0.8211)\n",
            "450 tensor(0.8105)\n",
            "500 tensor(0.8211)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1-GI96885FT",
        "colab_type": "code",
        "outputId": "e7c3096d-3a12-46ff-f154-f68219649c35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "list(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.2648, -0.2296, -0.2284, -0.0022, -0.0883, -0.1823, -0.1992, -0.1208],\n",
              "         [ 0.0762,  0.0549, -0.1130,  0.3622, -0.3106, -0.1887,  0.3761,  0.0787],\n",
              "         [ 0.3290, -0.2066, -0.2682, -0.0020, -0.1595, -0.2266, -0.1912,  0.3262],\n",
              "         [-0.3164,  0.3207, -0.1899, -0.3152,  0.2069,  0.0322, -0.0067,  0.0748],\n",
              "         [ 0.1319, -0.2061,  0.1027, -0.1297, -0.4038,  0.0167, -0.2884,  0.1112],\n",
              "         [ 0.0202, -0.2644, -0.0765, -0.1151, -0.0815, -0.2204,  0.2370,  0.1984],\n",
              "         [-0.2250, -0.1903, -0.3792,  0.3180,  0.3390, -0.0656, -0.0207, -0.0893],\n",
              "         [ 0.0228, -0.1490,  0.2443,  0.1154,  0.1085, -0.2102, -0.4178,  0.1029],\n",
              "         [ 0.2001,  0.3581, -0.2093, -0.0849, -0.1632, -0.0945, -0.2397,  0.0994],\n",
              "         [-0.1126, -0.0832, -0.3842, -0.1975, -0.0985, -0.2712, -0.2656, -0.1384],\n",
              "         [-0.1042, -0.3998,  0.0087, -0.1237, -0.2470, -0.2739,  0.0463,  0.1692],\n",
              "         [ 0.3339,  0.0103, -0.1999,  0.0058,  0.2897,  0.1250,  0.0312,  0.0589],\n",
              "         [-0.1601, -0.3936, -0.1111, -0.3682,  0.1466, -0.2378,  0.0582, -0.0299],\n",
              "         [ 0.2540, -0.3013, -0.0888,  0.1785,  0.2801, -0.0785, -0.0828, -0.1179],\n",
              "         [-0.1258, -0.0880, -0.1671,  0.0443,  0.1052, -0.0622, -0.1889,  0.2692],\n",
              "         [ 0.2877,  0.3061,  0.0137,  0.1306, -0.3053, -0.2607,  0.3237, -0.2656]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 0.2044,  0.0818, -0.1269, -0.1365,  0.2610,  0.1120, -0.0977, -0.1373,\n",
              "         -0.0041,  0.3252,  0.0113, -0.1850,  0.0706, -0.0147,  0.3809, -0.2966],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[ 9.8238e-02, -1.1088e-01,  1.8096e-01,  1.5131e-01,  1.3978e-01,\n",
              "          -2.8830e-02,  2.5768e-02,  1.9073e-01,  1.2250e-01,  2.8362e-02,\n",
              "           2.4684e-01, -7.3823e-02,  7.2895e-02, -6.8123e-02,  1.0621e-02,\n",
              "           1.5737e-01],\n",
              "         [ 1.5986e-01,  3.8011e-02, -1.3543e-01,  1.7226e-02,  1.5443e-01,\n",
              "           3.5995e-02, -6.6658e-02,  1.1560e-01, -2.6815e-01,  1.0667e-01,\n",
              "           3.3751e-05, -2.2186e-01, -2.9726e-01, -8.3572e-02,  4.0142e-03,\n",
              "          -7.4145e-03],\n",
              "         [ 2.5649e-01, -2.2193e-02, -3.8888e-02, -2.4655e-01,  1.3736e-01,\n",
              "          -1.9032e-01, -1.7930e-01,  2.7849e-01,  1.0456e-03, -1.3645e-01,\n",
              "          -5.3353e-02,  2.3324e-01,  2.9301e-01,  2.6169e-01,  2.1655e-01,\n",
              "           1.2052e-01],\n",
              "         [-1.3313e-01, -1.7123e-01,  6.9993e-02, -2.4614e-01,  1.7320e-01,\n",
              "           2.1677e-01,  1.2829e-01,  5.9277e-03, -6.1432e-02,  1.0515e-01,\n",
              "           1.8029e-03,  1.7730e-01, -1.2137e-01,  1.0860e-01,  2.9059e-02,\n",
              "           1.7007e-01],\n",
              "         [-1.0553e-01, -2.1140e-01, -1.2069e-01,  2.0548e-01,  9.0568e-02,\n",
              "          -2.3669e-01, -1.2334e-01,  1.9244e-01, -1.9016e-01, -1.1143e-01,\n",
              "           2.1080e-01, -2.0281e-01,  1.0473e-01,  2.2079e-01,  4.8091e-02,\n",
              "           7.4136e-02],\n",
              "         [ 1.9057e-01, -6.1560e-02,  2.2611e-01,  1.2189e-01, -1.9493e-01,\n",
              "           4.4961e-02, -2.1238e-01,  9.2445e-02, -9.6191e-02,  1.1627e-01,\n",
              "          -3.4796e-02, -1.8001e-01, -3.0877e-02, -2.4496e-01, -1.0658e-01,\n",
              "           4.6269e-02],\n",
              "         [-8.5863e-02,  1.2336e-01, -3.8386e-02, -1.2147e-01, -4.7591e-02,\n",
              "           1.5721e-01,  2.2456e-01, -4.3244e-02, -2.4583e-01,  7.2536e-02,\n",
              "           1.8658e-01,  2.3375e-01,  1.7652e-01, -1.2518e-01,  1.1347e-01,\n",
              "          -1.1372e-01],\n",
              "         [-1.1114e-01, -7.9707e-02,  1.4064e-02,  2.4685e-01, -4.8963e-02,\n",
              "          -1.6558e-01,  1.8912e-01, -4.4606e-02, -6.5232e-02, -2.0657e-01,\n",
              "           2.1968e-01, -4.3010e-03,  1.4815e-01, -1.4808e-01, -2.0586e-01,\n",
              "          -1.8406e-01],\n",
              "         [ 1.7398e-01,  1.8210e-02,  2.4644e-02, -4.1862e-03, -1.2900e-01,\n",
              "           6.7679e-02,  9.2232e-02, -2.2367e-01, -3.4588e-02, -3.6385e-02,\n",
              "           3.2993e-02, -6.0250e-03,  4.0422e-02, -2.2938e-03, -9.1905e-02,\n",
              "           2.1761e-01],\n",
              "         [ 2.8415e-01, -1.2044e-01,  1.0956e-01,  1.5150e-01, -8.3019e-02,\n",
              "          -2.1286e-01, -1.6428e-01,  3.1524e-01,  3.4401e-02, -5.0709e-02,\n",
              "           3.3535e-01,  6.0735e-02,  2.1328e-01,  2.0055e-01,  7.7896e-02,\n",
              "           1.5187e-01],\n",
              "         [ 2.5693e-01, -1.2408e-01,  2.4736e-01,  1.8550e-01, -1.4259e-01,\n",
              "           1.0177e-02, -3.8222e-03,  1.8506e-02, -1.0136e-01, -1.1017e-01,\n",
              "           1.6420e-01,  1.4533e-01,  3.0836e-01,  1.0559e-01,  1.8899e-01,\n",
              "          -1.4854e-01],\n",
              "         [ 1.1292e-01, -2.8416e-01, -6.1522e-02,  1.1273e-01,  2.8317e-01,\n",
              "           1.9327e-01, -1.1808e-01,  2.5529e-01, -2.6599e-01,  1.2172e-01,\n",
              "           8.6835e-02,  6.6342e-02,  9.3586e-02,  8.2939e-02,  8.6966e-02,\n",
              "          -1.6545e-01],\n",
              "         [-2.2439e-01,  1.0709e-01,  1.0318e-01, -2.4975e-01, -2.1983e-01,\n",
              "          -2.3350e-01, -2.3828e-01, -9.3772e-03, -1.1567e-01, -7.2282e-02,\n",
              "           2.4131e-01,  8.9372e-02,  2.4433e-02, -9.2072e-02, -1.7739e-01,\n",
              "          -1.5586e-01],\n",
              "         [-2.4008e-01, -1.7909e-01,  1.1109e-01, -7.4525e-02,  6.1141e-02,\n",
              "           1.9892e-01,  2.2166e-01, -1.3877e-01,  3.0671e-01,  1.2178e-01,\n",
              "          -3.3044e-01,  1.1176e-01, -2.8425e-01, -2.5123e-01, -9.0458e-02,\n",
              "          -1.8114e-01],\n",
              "         [ 5.0489e-02, -2.1976e-01,  9.2585e-02,  1.8444e-01, -1.4279e-01,\n",
              "           1.7364e-01,  8.0598e-02, -4.9423e-02, -2.7566e-01,  1.9141e-01,\n",
              "           3.6380e-02, -5.2303e-02,  3.1809e-01, -4.4117e-02,  2.2855e-01,\n",
              "           6.2778e-02],\n",
              "         [ 7.4603e-02, -3.5722e-02, -1.1846e-01, -9.7997e-02, -9.8315e-02,\n",
              "          -6.0075e-02, -1.0404e-02, -2.1163e-02, -2.4650e-01,  2.7054e-01,\n",
              "           3.2905e-01, -6.8555e-02, -1.3463e-01,  1.6269e-02,  1.1704e-01,\n",
              "           1.4826e-01],\n",
              "         [-2.2227e-01,  8.7362e-02, -5.5658e-02, -1.1675e-01, -1.8141e-01,\n",
              "           6.7071e-02, -2.8567e-01, -4.6159e-02, -1.6627e-01,  5.8058e-02,\n",
              "          -7.4058e-02, -2.4199e-01,  2.2380e-02,  7.2664e-02,  1.3889e-01,\n",
              "          -1.9813e-01],\n",
              "         [ 2.7427e-01, -3.0600e-02,  5.6768e-02,  2.2250e-01,  3.9872e-02,\n",
              "          -1.4078e-02,  6.3641e-02,  2.3324e-01, -1.2275e-02,  4.5330e-02,\n",
              "           3.0794e-01, -6.0998e-02,  1.5814e-01,  2.4361e-01,  1.7888e-01,\n",
              "          -2.7600e-01],\n",
              "         [-1.4398e-01,  1.4103e-01, -2.4748e-01,  2.6675e-01, -1.0294e-03,\n",
              "          -2.7891e-01,  2.1685e-01, -5.0824e-03,  2.0388e-02, -1.0407e-01,\n",
              "           7.4089e-02, -5.7933e-02, -3.4701e-02,  7.5940e-02, -1.4116e-01,\n",
              "           1.7995e-01],\n",
              "         [ 1.6481e-01,  1.0983e-01,  1.9111e-01, -1.2420e-01, -1.1024e-01,\n",
              "           3.7125e-02,  2.0939e-01,  3.9631e-02, -6.5217e-02, -2.2796e-01,\n",
              "          -1.3318e-01,  2.3212e-01,  6.0609e-02, -1.4207e-01,  1.2171e-01,\n",
              "          -6.1377e-02],\n",
              "         [ 1.9636e-01, -1.4999e-01,  2.8654e-02, -3.6025e-02,  1.9618e-01,\n",
              "          -6.0758e-02,  4.9181e-02,  1.6171e-01,  8.4606e-02,  1.7709e-01,\n",
              "           2.2061e-01, -1.1833e-01,  2.7311e-01,  7.6798e-03,  1.6134e-01,\n",
              "          -1.7337e-01],\n",
              "         [-7.0930e-02, -2.0513e-01, -1.0248e-01, -1.8052e-01, -2.4805e-01,\n",
              "          -1.1344e-01, -8.2097e-02, -1.6374e-01,  5.1141e-02,  9.8476e-04,\n",
              "           1.0229e-01, -1.9897e-01,  9.6362e-02, -1.7755e-01,  4.6570e-02,\n",
              "          -1.6603e-01]], requires_grad=True), Parameter containing:\n",
              " tensor([ 0.1066, -0.1903,  0.1570,  0.1110, -0.2039, -0.1313, -0.2450,  0.0599,\n",
              "         -0.2081,  0.0826,  0.1495,  0.0863,  0.0633,  0.1278,  0.0747,  0.0235,\n",
              "          0.0366,  0.0006,  0.2087, -0.2119,  0.0029, -0.0835],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[ 0.0224, -0.1285,  0.2267,  0.2399,  0.0921, -0.1149, -0.0371,  0.1275,\n",
              "           0.0785,  0.2381,  0.0947,  0.1799, -0.1232, -0.1003,  0.1343,  0.2162,\n",
              "           0.0805,  0.1316, -0.0857, -0.1894,  0.0635,  0.1051]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([0.2001], requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxhBZJ-4-xEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}